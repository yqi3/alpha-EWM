library(optimization)
source("5.2.1_Helper_Fns.R")
#### Read superpopulation data, specify number of MC trials and sample size ####
superpop <- read.csv("data_tau5.4.csv")
n_samples <- 2
n <- 1000
#### Simulation ####
for (alp in c(0.25,0.3,0.4,0.5,0.8,1)) {
## Match population truth and specify parameters for simulated annealing
if (alp == 0.25) {
Lor_value_true <- 2.261504
rate <- 0.6
n_limit <- 5
} else if (alp == 0.3) {
Lor_value_true <- 2.751794
rate <- 0.6
n_limit <- 5
} else if (alp == 0.4) {
Lor_value_true <- 3.758295
rate <- 0.6
n_limit <- 3
} else if (alp == 0.5) {
Lor_value_true <- 4.795932
rate <- 0.6
n_limit <- 3
} else if (alp == 0.8) {
Lor_value_true <- 8.112018
rate <- 0.6
n_limit <- 3
} else {
Lor_value_true <- 10.62597
rate <- 0.6
n_limit <- 3
}
set.seed(1234)
est_Lor <- numeric(n_samples)
frac_trt <- numeric(n_samples)
SEs <- numeric(n_samples)
coverage <- numeric(n_samples)
for (idx in 1:n_samples) {
print(paste0("alpha = ", alp, ", n = ", n, ", sample ", idx))
#### Prep Data ####
rand <- sample.int(1e6, n, replace = F)
datause <- superpop[rand,]
datause$p <- 2/3  # randomized study with propensity score=2/3
group <- make.cvgroup.balanced(datause, 2, 'A')  # for CF, K = 2
datause_g1 <- datause[group==1,]
datause_g2 <- datause[group==2,]
#### Linear Rule: Optimization over pi Using Simulated Annealing ####
counter <- 0
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing SA progress
print("Running SA for hat(pi)...")
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0,as.numeric(quantile(datause$Y,alp))), lower = c(-5,-5,-5,-5,-5,min(datause$Y)), upper = c(5,5,5,5,5,max(datause$Y)), control = list(r = rate, nlimit = n_limit))
#### Estimation & Inference ####
best_linear <- obj_sa$par
d_linear <- best_linear[1]+best_linear[2]*datause$X1+best_linear[3]*datause$X2+best_linear[4]*datause$X3+best_linear[5]*datause$X4>0
frac_trt[idx] <- mean(d_linear)
datause$d <- d_linear
## Re-evaluation Using Best Parameters
mu0 <- numeric(n)
mu1 <- numeric(n)
mu_g1 <- get_mu(best_linear[6], datause_g2)  # construct outcome nuisances using the other group
mu0[group==1] <- predict(mu_g1[[1]], datause_g1[,c(1:4)])$predictions  # predict on current group
mu1[group==1] <- predict(mu_g1[[2]], datause_g1[,c(1:4)])$predictions
mu_g2 <- get_mu(best_linear[6], datause_g1)
mu0[group==2] <- predict(mu_g2[[1]], datause_g2[,c(1:4)])$predictions
mu1[group==2] <- predict(mu_g2[[2]], datause_g2[,c(1:4)])$predictions
scores <- compute_scores(alp, datause$A, d_linear, datause$Y, mu1, mu0, datause$p, best_linear[6])
Lor_linear <- mean(scores)
est_Lor[idx] <- Lor_linear
Lor_var <- var(scores)/n
Lor_se <- sqrt(Lor_var)
SEs[idx] <- Lor_se
CI <- c(Lor_linear-1.96*Lor_se, Lor_linear+1.96*Lor_se)
## Coverage Rates
if (Lor_value_true >= CI[1] & Lor_value_true <= CI[2]) {
coverage[idx] <- 1
}
else {
coverage[idx] <- 0
}
## Print Results
print(paste0("Current est Lor: ", est_Lor[idx]))
print(paste0("Current var: ", Lor_var))
print(paste0("Mean SE: ", mean(SEs[1:idx])))
print(paste0("Median SE: ", median(SEs[1:idx])))
print(paste0("Avg est Lor: ", mean(est_Lor[1:idx])))
print(paste0("Avg est frac trt: ", mean(frac_trt[1:idx])))
print(paste0("Bias: ", mean(est_Lor[1:idx])-Lor_value_true))
print(paste0("Var: ", var(est_Lor[1:idx])))
print(paste0("SD: ", sd(est_Lor[1:idx])))
print(paste0("MSE: ", (mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx])))
print(paste0("Coverage: ", mean(coverage[1:idx])))
print("----------------")
}
## Save Results
assign(paste0("result_n",n,"_alpha",alp), as.data.frame(cbind("n"=n, "alpha"=alp, "avg_frac_trt"=mean(frac_trt[1:idx]), "bias"=mean(est_Lor[1:idx])-Lor_value_true, "variance"=var(est_Lor[1:idx]), "MSE"=(mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx]), "coverage"=mean(coverage[1:idx]))))
}
#### Show Final Results ####
result_n1000_alpha0.25
result_n1000_alpha0.3
result_n1000_alpha0.4
result_n1000_alpha0.5
result_n1000_alpha0.8
result_n1000_alpha1
######################################
# Author: Alice Yuan Qi
# Fan, Qi & Xu (2024), Table 5: n = 1500, alphas = {0.8, 1}
# Simulation results using data generating process in Section 5.2.1; tau is specified as (5.4) in paper
# For exact reproducibility of random forests using grf, this script should be run on the following platform
# R version 4.0.5 (2021-03-31)
# Platform: x86_64-pc-linux-gnu (64-bit)
# Running under: Rocky Linux 9.0 (Blue Onyx)
# For reasonable run time, we recommend running this script with at least 40 CPUs simultaneously, or breaking the for loop into smaller sub-tasks.
######################################
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(grf)
library(optimization)
source("5.2.1_Helper_Fns.R")
#### Read superpopulation data, specify number of MC trials, sample size, and parameters for simulated annealing ####
superpop <- read.csv("data_tau5.4.csv")
n_samples <- 1000
n <- 1500
rate <- 0.6
n_limit <- 5
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing SA progress
#### Simulation ####
for (alp in c(0.8,1)) {
## Match Population Truth
if (alp == 0.8) {
Lor_value_true <- 8.112018
} else {
Lor_value_true <- 10.62597
}
set.seed(1234)
est_Lor <- numeric(n_samples)
frac_trt <- numeric(n_samples)
SEs <- numeric(n_samples)
coverage <- numeric(n_samples)
for (idx in 1:n_samples) {
print(paste0("alpha = ", alp, ", n = ", n, ", sample ", idx))
#### Prep Data ####
rand <- sample.int(1e6, n, replace = F)
datause <- superpop[rand,]
datause$p <- 2/3  # randomized study with propensity score=2/3
group <- make.cvgroup.balanced(datause, 2, 'A')  # for CF, K = 2
datause_g1 <- datause[group==1,]
datause_g2 <- datause[group==2,]
#### Linear Rule: Optimization over pi Using Simulated Annealing ####
counter <- 0
print("Running SA for hat(pi)...")
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0,as.numeric(quantile(datause$Y,alp))), lower = c(-5,-5,-5,-5,-5,min(datause$Y)), upper = c(5,5,5,5,5,max(datause$Y)), control = list(r = rate, nlimit = n_limit))
#### Estimation & Inference ####
best_linear <- obj_sa$par
d_linear <- best_linear[1]+best_linear[2]*datause$X1+best_linear[3]*datause$X2+best_linear[4]*datause$X3+best_linear[5]*datause$X4>0
frac_trt[idx] <- mean(d_linear)
datause$d <- d_linear
## Re-evaluation Using Best Parameters
mu0 <- numeric(n)
mu1 <- numeric(n)
mu_g1 <- get_mu(best_linear[6], datause_g2)  # construct outcome nuisances using the other group
mu0[group==1] <- predict(mu_g1[[1]], datause_g1[,c(1:4)])$predictions  # predict on current group
mu1[group==1] <- predict(mu_g1[[2]], datause_g1[,c(1:4)])$predictions
mu_g2 <- get_mu(best_linear[6], datause_g1)
mu0[group==2] <- predict(mu_g2[[1]], datause_g2[,c(1:4)])$predictions
mu1[group==2] <- predict(mu_g2[[2]], datause_g2[,c(1:4)])$predictions
scores <- compute_scores(alp, datause$A, d_linear, datause$Y, mu1, mu0, datause$p, best_linear[6])
Lor_linear <- mean(scores)
est_Lor[idx] <- Lor_linear
Lor_var <- var(scores)/n
Lor_se <- sqrt(Lor_var)
SEs[idx] <- Lor_se
CI <- c(Lor_linear-1.96*Lor_se, Lor_linear+1.96*Lor_se)
## Coverage Rates
if (Lor_value_true >= CI[1] & Lor_value_true <= CI[2]) {
coverage[idx] <- 1
}
else {
coverage[idx] <- 0
}
## Print Results
print(paste0("Current est Lor: ", est_Lor[idx]))
print(paste0("Current var: ", Lor_var))
print(paste0("Mean SE: ", mean(SEs[1:idx])))
print(paste0("Median SE: ", median(SEs[1:idx])))
print(paste0("Avg est Lor: ", mean(est_Lor[1:idx])))
print(paste0("Avg est frac trt: ", mean(frac_trt[1:idx])))
print(paste0("Bias: ", mean(est_Lor[1:idx])-Lor_value_true))
print(paste0("Var: ", var(est_Lor[1:idx])))
print(paste0("SD: ", sd(est_Lor[1:idx])))
print(paste0("MSE: ", (mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx])))
print(paste0("Coverage: ", mean(coverage[1:idx])))
print("----------------")
}
## Save Results
assign(paste0("result_n",n,"_alpha",alp), as.data.frame(cbind("n"=n, "alpha"=alp, "avg_frac_trt"=mean(frac_trt[1:idx]), "bias"=mean(est_Lor[1:idx])-Lor_value_true, "variance"=var(est_Lor[1:idx]), "MSE"=(mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx]), "coverage"=mean(coverage[1:idx]))))
}
######################################
# Author: Alice Yuan Qi
# Fan, Qi & Xu (2024), Table 5: n = 1500, alphas = {0.8, 1}
# Simulation results using data generating process in Section 5.2.1; tau is specified as (5.4) in paper
# For exact reproducibility of random forests using grf, this script should be run on the following platform
# R version 4.0.5 (2021-03-31)
# Platform: x86_64-pc-linux-gnu (64-bit)
# Running under: Rocky Linux 9.0 (Blue Onyx)
# For reasonable run time, we recommend running this script with at least 40 CPUs simultaneously, or breaking the for loop into smaller sub-tasks.
######################################
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(grf)
library(optimization)
source("5.2.1_Helper_Fns.R")
#### Read superpopulation data, specify number of MC trials, sample size, and parameters for simulated annealing ####
superpop <- read.csv("data_tau5.4.csv")
n_samples <- 2
n <- 1500
rate <- 0.6
n_limit <- 5
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing SA progress
#### Simulation ####
for (alp in c(0.8,1)) {
## Match Population Truth
if (alp == 0.8) {
Lor_value_true <- 8.112018
} else {
Lor_value_true <- 10.62597
}
set.seed(1234)
est_Lor <- numeric(n_samples)
frac_trt <- numeric(n_samples)
SEs <- numeric(n_samples)
coverage <- numeric(n_samples)
for (idx in 1:n_samples) {
print(paste0("alpha = ", alp, ", n = ", n, ", sample ", idx))
#### Prep Data ####
rand <- sample.int(1e6, n, replace = F)
datause <- superpop[rand,]
datause$p <- 2/3  # randomized study with propensity score=2/3
group <- make.cvgroup.balanced(datause, 2, 'A')  # for CF, K = 2
datause_g1 <- datause[group==1,]
datause_g2 <- datause[group==2,]
#### Linear Rule: Optimization over pi Using Simulated Annealing ####
counter <- 0
print("Running SA for hat(pi)...")
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0,as.numeric(quantile(datause$Y,alp))), lower = c(-5,-5,-5,-5,-5,min(datause$Y)), upper = c(5,5,5,5,5,max(datause$Y)), control = list(r = rate, nlimit = n_limit))
#### Estimation & Inference ####
best_linear <- obj_sa$par
d_linear <- best_linear[1]+best_linear[2]*datause$X1+best_linear[3]*datause$X2+best_linear[4]*datause$X3+best_linear[5]*datause$X4>0
frac_trt[idx] <- mean(d_linear)
datause$d <- d_linear
## Re-evaluation Using Best Parameters
mu0 <- numeric(n)
mu1 <- numeric(n)
mu_g1 <- get_mu(best_linear[6], datause_g2)  # construct outcome nuisances using the other group
mu0[group==1] <- predict(mu_g1[[1]], datause_g1[,c(1:4)])$predictions  # predict on current group
mu1[group==1] <- predict(mu_g1[[2]], datause_g1[,c(1:4)])$predictions
mu_g2 <- get_mu(best_linear[6], datause_g1)
mu0[group==2] <- predict(mu_g2[[1]], datause_g2[,c(1:4)])$predictions
mu1[group==2] <- predict(mu_g2[[2]], datause_g2[,c(1:4)])$predictions
scores <- compute_scores(alp, datause$A, d_linear, datause$Y, mu1, mu0, datause$p, best_linear[6])
Lor_linear <- mean(scores)
est_Lor[idx] <- Lor_linear
Lor_var <- var(scores)/n
Lor_se <- sqrt(Lor_var)
SEs[idx] <- Lor_se
CI <- c(Lor_linear-1.96*Lor_se, Lor_linear+1.96*Lor_se)
## Coverage Rates
if (Lor_value_true >= CI[1] & Lor_value_true <= CI[2]) {
coverage[idx] <- 1
}
else {
coverage[idx] <- 0
}
## Print Results
print(paste0("Current est Lor: ", est_Lor[idx]))
print(paste0("Current var: ", Lor_var))
print(paste0("Mean SE: ", mean(SEs[1:idx])))
print(paste0("Median SE: ", median(SEs[1:idx])))
print(paste0("Avg est Lor: ", mean(est_Lor[1:idx])))
print(paste0("Avg est frac trt: ", mean(frac_trt[1:idx])))
print(paste0("Bias: ", mean(est_Lor[1:idx])-Lor_value_true))
print(paste0("Var: ", var(est_Lor[1:idx])))
print(paste0("SD: ", sd(est_Lor[1:idx])))
print(paste0("MSE: ", (mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx])))
print(paste0("Coverage: ", mean(coverage[1:idx])))
print("----------------")
}
## Save Results
assign(paste0("result_n",n,"_alpha",alp), as.data.frame(cbind("n"=n, "alpha"=alp, "avg_frac_trt"=mean(frac_trt[1:idx]), "bias"=mean(est_Lor[1:idx])-Lor_value_true, "variance"=var(est_Lor[1:idx]), "MSE"=(mean(est_Lor[1:idx])-Lor_value_true)^2+var(est_Lor[1:idx]), "coverage"=mean(coverage[1:idx]))))
}
#### Show Final Results ####
result_n1500_alpha0.8
result_n1500_alpha1
######################################
# Author: Alice Yuan Qi
# Fan, Qi & Xu (2024), Table 4: n = 1500, 6 alphas
# Population truths using data generating process in Section 5.2.1; tau is specified as (5.3) in paper
# For reasonable run time, we recommend running this script with at least 40 CPUs simultaneously, or breaking the for loop into smaller sub-tasks.
######################################
rm(list=ls())
library(optimization)
set.seed(1234)
#### Helper function that computes the true value of the objective function (generalized Lorenz) given some combination of linear policy parameters and eta, used for policy learning via simulated annealing ####
obj_linear <- function(coef) {
if (counter %% 100 == 0) {
print(paste0("Progress: ", 100*counter/total, "%"))  # show progress
}
counter <<- counter+1
d_pop <- coef[1]+coef[2]*superpop$X1+coef[3]*superpop$X2+coef[4]*superpop$X3+coef[5]*superpop$X4>0
outcomes <- (superpop$W==d_pop)*superpop$Y+(superpop$W!=d_pop)*superpop$Y_cf
return((-1)*alp*mean(outcomes[outcomes<=quantile(outcomes,alp)]))
}
#### Read superpopulation data ####
superpop <- read.csv("data_tau5.3.csv")
######################################
# Author: Alice Yuan Qi
# Fan, Qi & Xu (2024), Table 4: n = 1500, 6 alphas
# Population truths using data generating process in Section 5.2.1; tau is specified as (5.3) in paper
# For reasonable run time, we recommend running this script with at least 40 CPUs simultaneously, or breaking the for loop into smaller sub-tasks.
######################################
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(optimization)
set.seed(1234)
#### Helper function that computes the true value of the objective function (generalized Lorenz) given some combination of linear policy parameters and eta, used for policy learning via simulated annealing ####
obj_linear <- function(coef) {
if (counter %% 100 == 0) {
print(paste0("Progress: ", 100*counter/total, "%"))  # show progress
}
counter <<- counter+1
d_pop <- coef[1]+coef[2]*superpop$X1+coef[3]*superpop$X2+coef[4]*superpop$X3+coef[5]*superpop$X4>0
outcomes <- (superpop$W==d_pop)*superpop$Y+(superpop$W!=d_pop)*superpop$Y_cf
return((-1)*alp*mean(outcomes[outcomes<=quantile(outcomes,alp)]))
}
#### Read superpopulation data ####
superpop <- read.csv("data_tau5.3.csv")
superpop$Y_cf <- (superpop$W==1)*(superpop$Y-superpop$tau) + (superpop$W==0)*(superpop$Y+superpop$tau)  # compute true counterfactuals
superpop$Y_cf <- (superpop$A==1)*(superpop$Y-superpop$tau) + (superpop$A==0)*(superpop$Y+superpop$tau)  # compute true counterfactuals
#### Learn true linear rule: pi=1 if LES>0 ####
## Specify parameters for simulated annealing
rate <- 0.6
n_limit <- 10
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing progress
## Run SA for superpopulation
for (alp in c(0.25,0.3,0.4,0.5,0.8,1)) {
counter <- 0
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0), lower = c(-5,-5,-5,-5,-5), upper = c(5,5,5,5,5), trace = TRUE, control = list(r = rate, nlimit = n_limit))
print(paste0("alpha = ", alp, ", true Lorenz = ", -obj_sa$function_value))
}
#### Helper function that computes the true value of the objective function (generalized Lorenz) given some combination of linear policy parameters and eta, used for policy learning via simulated annealing ####
obj_linear <- function(coef) {
if (counter %% 100 == 0) {
print(paste0("Progress: ", 100*counter/total, "%"))  # show progress
}
counter <<- counter+1
d_pop <- coef[1]+coef[2]*superpop$X1+coef[3]*superpop$X2+coef[4]*superpop$X3+coef[5]*superpop$X4>0
outcomes <- (superpop$A==d_pop)*superpop$Y+(superpop$A!=d_pop)*superpop$Y_cf
return((-1)*alp*mean(outcomes[outcomes<=quantile(outcomes,alp)]))
}
superpop$Y_cf <- (superpop$A==1)*(superpop$Y-superpop$tau) + (superpop$A==0)*(superpop$Y+superpop$tau)  # compute true counterfactuals
#### Learn true linear rule: pi=1 if LES>0 ####
## Specify parameters for simulated annealing
rate <- 0.6
n_limit <- 10
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing progress
## Run SA for superpopulation
for (alp in c(0.25,0.3,0.4,0.5,0.8,1)) {
counter <- 0
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0), lower = c(-5,-5,-5,-5,-5), upper = c(5,5,5,5,5), trace = TRUE, control = list(r = rate, nlimit = n_limit))
print(paste0("alpha = ", alp, ", true Lorenz = ", -obj_sa$function_value))
}
#### Helper function that computes the true value of the objective function (generalized Lorenz) given some combination of linear policy parameters and eta, used for policy learning via simulated annealing ####
obj_linear <- function(coef) {
if (counter %% 10000 == 0) {
print(paste0("Progress: ", 100*counter/total, "%"))  # show progress
}
counter <<- counter+1
d_pop <- coef[1]+coef[2]*superpop$X1+coef[3]*superpop$X2+coef[4]*superpop$X3+coef[5]*superpop$X4>0
outcomes <- (superpop$A==d_pop)*superpop$Y+(superpop$A!=d_pop)*superpop$Y_cf
return((-1)*alp*mean(outcomes[outcomes<=quantile(outcomes,alp)]))
}
#### Learn true linear rule: pi=1 if LES>0 ####
## Specify parameters for simulated annealing
rate <- 0.85
n_limit <- 800
total <- ceiling(log(0.1/1000,rate))*n_limit  # for showing progress
## Run SA for superpopulation
for (alp in c(0.25,0.3,0.4,0.5,0.8,1)) {
print("Learning true optimal policy for alpha = ", alp, "...")
counter <- 0
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0), lower = c(-5,-5,-5,-5,-5), upper = c(5,5,5,5,5), trace = TRUE, control = list(r = rate, nlimit = n_limit))
print(paste0("alpha = ", alp, ", true Lorenz = ", -obj_sa$function_value))
}
## Run SA for superpopulation
for (alp in c(0.25,0.3,0.4,0.5,0.8,1)) {
print(paste0("Learning true optimal policy for alpha = ", alp, "..."))
counter <- 0
obj_sa <- optim_sa(fun = obj_linear, start = c(0,0,0,0,0), lower = c(-5,-5,-5,-5,-5), upper = c(5,5,5,5,5), trace = TRUE, control = list(r = rate, nlimit = n_limit))
print(paste0("alpha = ", alp, ", true Lorenz = ", -obj_sa$function_value))
}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
write.csv(signal_wgan_T, "signal_wgan_T_P.csv")
write.csv(signal_wgan_C, "signal_wgan_C_P.csv")
signal_wgan_T <- read.csv("signal_wgan_T_P.csv")
signal_wgan_C <- read.csv("signal_wgan_C_P")
signal_wgan_C <- read.csv("signal_wgan_C_P.csv")
View(signal_wgan_C)
View(signal_wgan_T)
rownames(signal_wgan_T) <- signal_wgan_T[1,]
rownames(signal_wgan_T) <- toString(signal_wgan_T[1,])
rownames(signal_wgan_T)
signal_wgan_T[1,]
rownames(signal_wgan_T) <- toString(signal_wgan_T$X)
signal_wgan_T$X
######################################
# Author: Alice Yuan Qi
# Figures 1 & 2 in Fan, Qi & Xu (2024)
# Optimal treatment regions, 6 alpha values Ã— 2 ITR classes = 12 plots
######################################
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(ggplot2)
path <- "Figs_1&2_plots/"  # for saving plots
path_ITR <- "Tables_1&7_ITR_para/"  # for reading policy parameters
plot_ub <- 20000  # for better visualization
for (alp in c(0.25, 0.3, 0.4, 0.5, 0.8, 1)) {
linear_para <- read.csv(paste0(path_ITR, alp,"_linear_para.csv"))
cubic_para <- read.csv(paste0(path_ITR, alp,"_cubic_para.csv"))
### Linear rule
b0 <- linear_para$b0
b1 <- linear_para$b1
b2 <- linear_para$b2
slope <- -b1/b2
intercept <- -b0/b2
plot_data <- read.csv("plot_data.csv")  # helper dataframe for plotting
jpeg(file=paste0(path, alp, "linear.jpeg"), width=6, height=4, units="in", res=200)
if (b2 < 0){
# Line is upper bound
plot_data$ub <- (slope*plot_data$edu+intercept>=0)*(slope*plot_data$edu+intercept)
plot_data$ub <- (plot_data$ub>=plot_ub)*plot_ub + (plot_data$ub<plot_ub)*plot_data$ub
plot_data <- rbind(plot_data, c((plot_ub-intercept)/slope, 0, 0, plot_ub))  # add helper point for plotting trt region
plot_data <- rbind(plot_data, c((0-intercept)/slope, 0, 0, 0))  # add a helper point for plotting trt region
print(ggplot(plot_data, aes(edu, prevearn)) +
geom_point(aes(size=n)) +
scale_size_continuous(range = c(-1, 8)) +
geom_ribbon(data=plot_data, aes(ymin = 0, ymax = ub), fill = "yellow", alpha = .2) +
ylim(0,plot_ub) +
xlim(7,18) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(title=paste0("Linear class, alpha=", alp)))
} else {
# Line is lower bound
plot_data$lb <- (slope*plot_data$edu+intercept>=0)*(slope*plot_data$edu+intercept)
plot_data <- rbind(plot_data, c((plot_ub-intercept)/slope, 0, 0, plot_ub))  # add helper point for plotting trt region
plot_data <- rbind(plot_data, c((0-intercept)/slope, 0, 0, 0))  # add a helper point for plotting trt region
print(ggplot(plot_data, aes(edu, prevearn)) +
geom_point(aes(size=n)) +
scale_size_continuous(range = c(-1, 8)) +
geom_ribbon(data=plot_data, aes(ymin = lb, ymax = plot_ub), fill = "yellow", alpha = .2) +
ylim(0,plot_ub) +
xlim(7,18) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(title=paste0("Linear class, alpha=", alp)))
}
dev.off()
### Cubic rule
b0 <- cubic_para$b0
b1 <- cubic_para$b1
b2 <- cubic_para$b2
b3 <- cubic_para$b3
b4 <- cubic_para$b4
# Make a helper grid for plotting
edu_grid <- seq(7,18,0.005)
helper <- data.frame("edu"=edu_grid, "prevearn"=rep(0,length(edu_grid)), "n"=rep(0,length(edu_grid)))
plot_data <- read.csv("plot_data.csv")
plot_data <- rbind(plot_data, helper)
intercept <- -b0/b2
coef_1 <- -b1/b2
coef_2 <- -b3/b2
coef_3 <- -b4/b2
jpeg(file=paste0(path, alp, "cubic.jpeg"), width=6, height=4, units="in", res=200)
if (b2 < 0) {
# Curve is upper bound
plot_data$ub <- (coef_1*plot_data$edu+coef_2*(plot_data$edu)^2+coef_3*(plot_data$edu)^3+intercept>=0)*(coef_1*plot_data$edu+coef_2*(plot_data$edu)^2+coef_3*(plot_data$edu)^3+intercept)
plot_data$ub <- (plot_data$ub>=plot_ub)*plot_ub + (plot_data$ub<plot_ub)*plot_data$ub
print(ggplot(plot_data, aes(edu, prevearn)) +
geom_point(aes(size=n)) +
scale_size_continuous(range = c(-1, 8)) +
geom_ribbon(data=plot_data, aes(ymin = 0, ymax = ub), fill = "yellow", alpha = .2) +
ylim(0, plot_ub) +
xlim(7,18) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(title=paste0("Linear class with squared and cubic edu, alpha=", alp)))
} else {
# Curve is lower bound
plot_data$lb <- (coef_1*plot_data$edu+coef_2*(plot_data$edu)^2+coef_3*(plot_data$edu)^3+intercept>=0)*(coef_1*plot_data$edu+coef_2*(plot_data$edu)^2+coef_3*(plot_data$edu)^3+intercept)
print(ggplot(plot_data, aes(edu, prevearn)) +
geom_point(aes(size=n)) +
scale_size_continuous(range = c(-1, 8)) +
geom_ribbon(data=plot_data, aes(ymin = lb, ymax = plot_ub), fill = "yellow", alpha = .2) +
ylim(0, plot_ub) +
xlim(7,18) +
theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(title=paste0("Linear class with squared and cubic edu, alpha=", alp)))
}
dev.off()
}
